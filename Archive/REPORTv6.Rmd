---
title: "Understanding Data and its Environment: Report assessment"
author: "Eugeni Vidal"
date: March 15, 2018
output:
  word_document: default
  pdf_document: default
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
  slidy_presentation:
bibliography: references.bib
---

# 1. Introduction

<Aim>
This report aims to describe, pre-process and analyse a set of data based on historical sales data collected from a nationwide retailer in the U.S and several external factors, so as to lead to the development of accurate predictive models. 

<Method>
The methodology is divided into six different steps, from describing the data to building and assessing the models developed. Each step is taken in order, although the whole process has to be understood as a set of nested loops rather than a straight line (see figure 1).

The whole process has been done using the software R and it is reproducible based on code available at https://github.com/eugenividal/Understanding-data-report.

# 2. Activate libraries and load data 

```{r, include=FALSE, cache=FALSE}
# install.packages("tidyverse")
# install.packages("dplyr")
# install.packages("psych")
# install.packages("lubridate")
# install.packages("caret")
# install.packages("car")

library(tidyverse)
library(dplyr)
library(psych)
library(lubridate)
library(caret)
library(car)
```

```{r, include=FALSE, cache=FALSE}
# Load data
source("code/load-data.R")
```

# 2. Data description

# 3. Joining the model data set
```{r}
library(dplyr)
# Join store-level data onto training dataset (so we know size and type)
train_joined = left_join(train, y= stores)
# Join train_joined onto features dataset (so we know the rest of variables)
train_joined = left_join(train_joined, y= features)
```

# 4. Cleaning and fixing problems with the data
```{r}
# Recode stores type based on size (some errors detected)
train_joined$Type[(train_joined$`Size (sq ft)`<50000)] <- "C"
train_joined$Type[(train_joined$`Size (sq ft)`>=50000) & (train_joined$`Size (sq ft)`<150000)] <- "B"
train_joined$Type[(train_joined$`Size (sq ft)`>=150000)] <- "A"
```

```{r}
# Delete promotions because 50% data are missing
train_joined$Promotion1 <- NULL
train_joined$Promotion2 <- NULL
train_joined$Promotion3 <- NULL
train_joined$Promotion4 <- NULL
train_joined$Promotion5 <- NULL
```

```{r}
# Remove negative values, although  they can be returns (to apply the log)
train_joined <- train_joined[train_joined$Weekly_Sales > 0, ]
```

```{r}
# Convert IsHoliday to numeric
train_joined$IsHoliday <- as.numeric(train_joined$IsHoliday)
```

```{r}
# Convert date info in format 'dmy'
train_joined$Date <- dmy(train_joined$Date)
```

# 5. Variables tranformation

```{r}
# Create a week number of the year
train_joined$WeekNum <- as.numeric(format(train_joined$Date+3,"%U"))
train_joined$year = lubridate::year(train_joined$Date)
```

```{r}
prevyear = select(train_joined, year, WeekNum, Dept, Store, old_sales = Weekly_Sales)
prevyear$year = prevyear$year + 1
summary(prevyear$year)
# Create variable Sales previous year
train_oldsales = left_join(train_joined, prevyear)
plot(train_oldsales$Weekly_Sales, train_oldsales$old_sales)
```

```{r, include=FALSE, cache=FALSE}
# Move location of columns
train_joined <- train_joined[c(13,2,1,6,7,3,12,5,4,8,9,10,11)]
```

```{r}
# Glance at the first 6 rows of the model dataset
head(train_joined)
```

# 6. Build the model

While developing the model, iteratively analyse the variables for:

1.Normality of distribution
2.Extreme values
3.Multiple colinearity
4.Monoscedasticity (even distribution of residuals)
5.p-value of coefficients and R2?F statistic of the model

```{r}
# Select a subset of numeric variables for regression modelling
train_joined.sel <- subset(train_joined, select = c(5,9,10, 11,12,13))
```

### 6.1 Check for non linearity (visually) and transform variables

```{r}
# Plot a correlation plot
pairs.panels(train_joined.sel, col="red")
```

```{r}
# Log transformation of Weekly sales to have a more normal distribution
train_joined.sel$Weekly_Sales <- log10(train_joined.sel$Weekly_Sales)
```

Other variables are not normal.

```{r}
# Plot a correlation plot
pairs.panels(train_joined.sel, col="red")
```

```{r message=FALSE}
#subset the data into train and test
n = nrow(train_joined.sel)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
dataT = train_joined.sel[trainIndex ,]
dataV = train_joined.sel[-trainIndex ,]
```

### 6.2. Check exteme values

Not many founded and there are so many observation that take some of is insignificant.

```{r}
# Fit the model (1)
fit <- lm(Weekly_Sales ~., data= dataT)
summary(fit) #R2 = 11,0%
```

### 6.3. Check for multi-collinearity with Variance Inflation Factor

Correlated: none VIF=1, moderately 1<VIF<5, ** highly 5<VIF<10, ...
```{r}
# Variance Inflation Factor
vif(fit)
```

```{r}
# Refit the model (2) - drop CPI and Fuel Price
fit <- lm(Weekly_Sales ~ `Size (sq ft)` + Temperature + Unemployment, data= dataT)
summary(fit) #R2 = 10,0%
```

```{r}
# Variance Inflation Factor
vif(fit)
```
VIF, F-ratio and p-values say it is good, so no need to do anything else.

### 6.4. Monoscedasticity (even distribution of residuals)

### 6.5. p-value of coefficients and R2?F statistic of the model