---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: ~/Dropbox/miscelanea/svm-r-markdown-templates/svm-latex-ms.tex
title: "Understanding Data and its Environment: Report assessment"
author:
- name: Eugeni Vidal
keywords: "pandoc, r markdown, knitr"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
biblio-style: apsr
endnote: no
bibliography: My library.bib
---

\pagebreak

```{r,echo=FALSE,message=FALSE,warning=FALSE}
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

# Introduction

The cambridge Dictionary describes sales forecasting as "the statement of what the amount or value of a company's sales is likely to be in the future, based on information available now about the market, past sales, etc"[@cambridge_dictionary_cambridge_nodate].

The increase of competition, complexity in business tasks, and the fact that nowadays circumstances, in general, tend to change more rapidly makes increasingly important and necessary for companies the use of forecasting technics for the prediction of their future prospects [@lancaster_forecasting_1985, p1].

<Aim>
This report aims to describe, pre-process and analyse a set of data based on historical sales data collected from a nationwide retailer in the U.S as well as on external factors, so as to lead to the development of an accurate predictive model. 

<Method>
The methodology followed is divided into 7 different steps, from describing the data to building and assessing the model developed (see figure 1). Notice that although each step is taken in order, the whole process has to be understood as a set of nested loops rather than a straight line.

![Methodology diagram \label{}](images/Methodology diagram.jpg)

<br>

The whole process has been done using the open software R and it is reproducible based on code available at https://github.com/eugenividal/Understanding-data-report.

This report is written as clearly and easily as possible, with the pretense that any person, without much prior knowledge in forecasting or in the R software, can understand it. For this purpose, the document describes not only the statistical process followed, but also the code used with the software R. 

<br>

# Data description

The first thing to start a project with the R software is to install the packages that we will need and load their libraries. To do this we use the code shown below. 

```{r,  message=FALSE, cache=FALSE}
# Activate libraries
library(tidyverse)
library(VIM)
library(dplyr)
library(psych)
library(lubridate)
library(caret)
library(car)
```

Secondly, we will load the data into the R environment.

```{r, message=FALSE, cache=FALSE}
# Load data
stores = read_csv("data/stores.csv")
features = read_csv("data/features.csv")
test = read_csv("data/test.csv")
train = read_csv("data/train.csv")
```

<br>

We are provided with 4 data sets (stores, features, train, and test). All of them with the same format: comma-separated values (csv). 

Below is a brief description of each of the datasets and their variables:

<Describe the type of each variable (categorical, numeric, etc)?>

**stores.csv (45 obs. of 3 variables)**

- Store: the anonymised store number <numeric> 
- Type: tore type, A: supercentre, B: superstore, C: supermarket <character>
- Size (sq ft): store size (in square feet) <numeric>

**features.csv (8,190 obs. of 12 variables)**

- Store: the anonymised store number <numeric>
- Date:	the week with the dated Friday <character>
- Temperature: average temperature in the region <numeric>
- Fuel_Price: cost of fuel in the region <numeric>
- Promotions:	anonymised data related to promotions, mainly price reductions that the retailer is running <numeric>
- CPI: the consumer price index <numeric>
- Unemployment: the unemployment rate <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**train.csv (421,570 obs. of 5 variables)**

- Store: the anonymised store number <numeric>
- Department: the anonymised department number <numeric>
- Date: the week with the dated Friday <character>
- Weekly_Sales: sales for the given department in the given store <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**test.csv (115,069 obs. of 5 variables)**

The validation dataset has the same fields as the train.csv, except we need to predict the weekly sales for each triplet of store, department, and date from 02/11/2012 to 26/07/2013.

<br>

Variables of each dataset have been explored through counts, summary statistics, crosstabulation and visualisations in order to identify potential inconsistences or problems in the data.

The main problems detected are briefly described below.

**1. Inconsistent data encoding**. In the stores dataset, some stores might be wrongly classified. Two stores under 50,000 sq ft are coded as type B and other two as type A, when presumably small size stores (<50000 sq ft) should be type C. 

```{r, include=FALSE, cache=FALSE}
# Crosstab Size and Type store data set
crosstab_Size_Type <- table(stores$Type, stores$`Size (sq ft)`)
# Print first 10 rows of mydata
head(crosstab_Size_Type, n=10)
```

**2. Missing values**. In the features dataset, 24,040 values are missing, almost 50% in the promotion variables and 7% in the `CPI` and `Unemployment` ones. With the function of the VIM package aggr() we can visualise them for each variable alone and for each combination of variables.

```{r , fig.cap=c("Missing values"), fig.height=3.5, fig.width=5.5, echo=TRUE}
# Plot missing values
aggr(features, prop=FALSE, col = "grey", cex.lab = 0.85, cex.axis = 0.65, cex.main = 0.85)
```

There are not missing values in the ret of the datasets.

**3. Negative values**. Some of the values in the promotions variables are negative: 4 in `Promotion1`, 25 in `Promotion2`, 13 in `Promotion3`, and 2 in `Promotion5`. Also within the `Weekly Sales` (1286 of 421571 (0.3%)) variable in the train dataset.

**4. Class type errors**. The  class of the `Date` variable within the train, features and test datasets is character instead of date. This doesn't allow to sort the data properly. Also in terms of class, it would be useful to have the variable IsHoliday in numeric type (1 or 0) instead of boolean (true or false). 

**5. Data not normally distributed**. Finally, to mentione that the data in some variables is not normally distributed. Weekly Sales data is clearly left-skewed as can be seen in the histogram below. But also Size have a not normal distribution, rather comb, and Fuel Price and CPI a bimodal. 

```{r,include=FALSE, cache=FALSE}
# Scientific notation Off
options(scipen=999)
```

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Plot a histogram
hist(train$Weekly_Sales, cex.lab = 0.85, cex.axis = 0.85, cex.main = 0.85, col = "grey", 
main=NULL, xlab="Weekly Sales", ylab="Frequency")
```

<br>

<Extreme values?. Here, in fixing problems or fiting the model?>

```{r,include=FALSE, cache=FALSE}
# Scientific notation On
options(scipen=001)
```

<br>

# Data joining

The next step is to join the stores, train and features datasets in a single one. This will allow us to make the analysis simpler and more straightforward. 

The test dataset will be left aside to gauge the effectiveness of the models once build.

To performance the join, we apply a left join. We chose this type of join because we want to end up having the same number of rows as the train dataset which contains the variable we want to predict `Weekly Sales`.  

First, we link the stores dataset with the train one, using the common attribute `Store`; then the resultant dataset with the features dataset, using the common attributes `Store`, `Date` and `IsHoliday`. See the code below.

```{r}
# Join store-level data onto training dataset (so we know size and type)
data_joined = left_join(train, y= stores)
# Join train_joined onto features dataset (so we know the rest of variables)
data_joined = left_join(data_joined, y= features)
```

The result is a data frame with 421,570 observations of 16 variables. Notice that 1,755 observations don't match between train and features. This is because we did a left join and the features datasets collect data until 2013-07-26 while the train dataset only until 2013-07-26. This way, we leave behind 1,755 observations and the 585 NA's previously detected in `CPI`, `Unemployment` and `IsHoliday`.

<br>

# Cleaning and fixing problems with the data

In this section, we will clean and fix inconsistencies or potential problems identified and already mentioned in the section data description. 

## Resolving inconsistent data encoding

We will assume that the type of store is based on size and consequently, will recode all stores under 50000 sq ft as C type. This way we solve the potential inconsistency of 4 stores under 5000 sq ft coded as type A and B.

```{r}
# Recode `stores$Type` based on `stores$Size (sq ft)`
data_joined$Type[(data_joined$`Size (sq ft)`<50000)] <- "C"
data_joined$Type[(data_joined$`Size (sq ft)`>=50000) & (data_joined$`Size (sq ft)`
<150000)] <- "B"
data_joined$Type[(data_joined$`Size (sq ft)`>=150000)] <- "A"
```

## Dealing with missing values

There are several approaches when we find missing data: delete the cases containing missing data, or replace (impute) the missing values with reasonable alternative data values [@kabacoff_r_2011, p353].

Deciding how to treat missing values will depend on your estimation of which procedures will produce the most reliable and accurate results [@kabacoff_r_2011, p.354].

"There is no established cutoff from the literature regarding an acceptable percentage of missing data in a data set for valid statistical inferences" [@dong_principled_2013]. Schafer [-@schafer_multiple_1999] argues
that a missing rate of 5% or less is inconsequential, while Bennett [-@bennett_how_2001] considers that more than 10% of values missing is likely to  biased the statistical analysis.

Given that the percentage of missing values in promotions is around 50%, we consider wil will apply listwise deletion, in other words, we will not consider them in the statistical analysis and will delete them from the dataset.

```{r}
# Delete promotions
data_joined$Promotion1 <- NULL
data_joined$Promotion2 <- NULL
data_joined$Promotion3 <- NULL
data_joined$Promotion4 <- NULL
data_joined$Promotion5 <- NULL
```

The missing values identified in the `CPI` and `Unemployment` variables disappeared when the datasets were merged. With the left_join, mentioned before, the observations that did not match with the train data set were delated and among them were the NAs identified in these variables.

## Interpretation of negative values

After analyzing the negative values of the `Weekly_Sales` variable we have concluded that they are returned products from previous weeks. So, no changes will be done in this sense. 

## Data type conversion

In order to make the variable `IsHoliday` more manageable, we will convert the data type into numeric using the general function `as.numeric()`.

```{r}
# Convert IsHoliday to numeric
data_joined$IsHoliday <- as.numeric(data_joined$IsHoliday)
```

We will also convert the `Date` class into a more appropiate format  such as date. To do that, we will use the `mdy ()` function of the lubridate package.

```{r message=FALSE}
# Convert date info to date format 'dmy'
data_joined$Date <- dmy(data_joined$Date)
```

## Consideration of data normalisation

It might be useful to have the errors normally distributed with constant variance in order to produce prediction intervals, although it is not considered necessary for forecasting [@hyndman_forecasting:_2017]. So, at this stage at least, we will keep all the variables with their original distribution. It could be preferable since the model would be more understandable and interpretable. 
<br>

# Data transformation

Once the inconsistencies or potential problems are fixed, we will make some transformations in order to prepare the dataset for building the  model.

First, we will create a week number of year in order to compare them.

```{r}
# Create a week number of the year variable
data_joined$WeekNum <- as.numeric(format(data_joined$Date+3,"%U"))
```

Secondly, We will create a year variable.

```{r}
# Create a year variable
data_joined$year = lubridate::year(data_joined$Date)
```

Thirdly, we will create a variable of previous years. The previous year's sales can be one of the most powerful predictors in sales forecasting [@zoltners_complete_2001]. 

```{r, echo=TRUE, fig.cap=c("Correlation Weekly Sales ~ Weekly Sales"), fig.height=3.5, fig.width=5.5}
# Create a previous year sales variable
prevyear = select(data_joined, year, WeekNum, Dept, Store, prev_Weekly_Sales = Weekly_Sales)
prevyear$year = prevyear$year + 1
# Create variable Sales previous year
data_joined = left_join(data_joined, prevyear)
```

As we can see in the following graph, Weekly Sales of the given year are highly correlated with the variable sales of the previous year created. 

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Plot correlation Weekely Sales ~ Previous Year Weekly Sales
plot(data_joined$Weekly_Sales, data_joined$prev_Weekly_Sales,col = "grey", main=NULL, 
     xlab="Weekly Sales", ylab="Previous year Weekly Sales")
```

<br>

Due to the fact that the new variable is based on sales of the previous year, there will be one year of missing values. 

<We will handle with this applying again listwise deletion. This will reduce the sample size by 38% (from 421,570 to 261,541) and consequently could reduce statistical power of our model dataset. However, an approach with the entire dataset (including the cases with missing data) could bias the results of the subsequent analysis [@bennett_how_2001, p.464].>

Install.packages("Hmisc")
library(Hmisc)

?impute

```{r}
#data_joined$prev_Weekly_Sales <- #as.numeric(impute(data_joined$prev_Weekly_Sales, mean))
```

```{r}
# Delete rowns with NA's
data_joined = na.omit(data_joined)
```

<The next step is to line-up holidays from one year and the other.>

```{r,include=FALSE, cache=FALSE}
# Match holiday weeks to holiday weeks. Adjust for Easter and adjust for the different Christmas week 

```

```{r,include=FALSE, cache=FALSE}
# Because of week mismatches between years take average of previous years sales 

```

Finally, we will rearrange the dataset in the most logical and comfortable way for us to build the model.

```{r, include=FALSE, cache=FALSE}
# Reorganise the columns
data_joined <- data_joined[c(2,1,6,7,3,4,14,8,9,10,11,12,5)]
```

The `head()` function allows us to have a glimpse of how the final model dataset looks like.

```{r}
# Glance at the first 6 rows of the model dataset
head(data_joined)
```
<br>

# Data partition

In this section, the model dataset is divided into two parts. The first one, the training set (data_joinedT), will be used to build the model; and the second one, the validation set (data_joinedV), to adjust it. 

<The test set that we put aside before, will be used to gauge the effectiveness of the models when applied to unseen data>. 

<why 80%? (reference needed)>

For the partition, we will use the `caret` package.

```{r message=FALSE}
#subset the data into train and test
n = nrow(data_joined)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
data_joinedT = data_joined[trainIndex ,]
data_joinedV = data_joined[-trainIndex ,]
```

<br>

# Building the model

## The choice of the technique

The choice of forecasting technique is a major consideration. There are many different methods to apply, from pure guesswork to highly complex mathematical analysis [@lancaster_forecasting_1985, p.15].

Three factors are determinant on the decision: accuracy, time-scale and cost [@lancaster_forecasting_1985, p.37-38].

We will opt for a multiple simple regression apporach for two fundamental reasons:

- Multiple regression is probaly the most understandable and accessible forecasting technique

- Multiregression regression is quicker and cheaper as well.

Multiple linear regression is described an statistical technique for "predicting a quantitative response (or dependent) variable from two or more explanatory (or independent) variables [@kabacoff_r_2011, p.175].


Its general form is:

<center><span style="font-size:larger;">*y~i~ = B~0~ + B~1~x~1~,~i~ + B~2~x~2~,~i~+...+ B~k~x~k~,~i~ + e~i~,*</span></center>

Where,

- <span style="font-size:larger;">*y~i~*</span></center> is the dependent variable (or variable to be forecast)
- <span style="font-size:larger;">*x~i~,~i~...x~k~,~i~*</span></center> are the independent variables (or predictors)
- <span style="font-size:larger;">*B~0~*</span></center> is the y-intercept
- <span style="font-size:larger;">*B~1~,...B~k~*</span></center> are the coefficients that measure the marginal effects of the predictors
- <span style="font-size:larger;">*e*</span></center> are the residuals (a random variable that captures the fact that regression models typically do not fit the data perfectly).

In our model, Weekly Sales would be the dependent variables and the rest of variables the predictors or independent variables.

## The selection of the predictors

To build the model we will performe a stepwise selection of variables by backwards elimination. This means we will start fitting a *full model* with all the candidate variables, and will progressively drop those variables which are not suitable or contribute to the sales prediction.

To fit the full model we will use the R basic function `lm ()`. 

```{r}
# Fit the model (1)
fit <- lm(Weekly_Sales ~ Type + `Size (sq ft)`+ prev_Weekly_Sales + Temperature + Fuel_Price + CPI + Unemployment + IsHoliday , data= data_joinedT)
summary(fit)
```

At a glance from the summary of the fit we can see that the model looks good:

All predictors have a low p-value, what means that they could be siginficant.

The R-squared is very high (96.74%) which says how well the model explains the variation in the data.

However, additional tests are needed. 

### Normalisation of the data (should be in identification of ptoblems and fixing the problems)

The first thing we did was to tranfor the data in log, to get a better normalization. However, when doing the validation the result was worse

```{r}
data_joinedTlog <- data.frame(log.Weekly_Sales = log10 (data_joinedT$Weekly_Sales+1), log.Size= log10 (data_joinedT$`Size (sq ft)`+1), log.prev_Weekly_Sales= log(data_joinedT$prev_Weekly_Sales+1), Temperature = data_joinedT$Temperature, log.Fuel_Price = log10 (data_joinedT$Fuel_Price), log.CPI = log10 (data_joinedT$CPI+1), Unemployment = data_joinedT$Unemployment, IsHoliday = data_joinedT$IsHoliday)
# pairs(data_joinedTlog)
```
```{r}
data_joinedVlog <- data.frame(log.Weekly_Sales = log10 (data_joinedV$Weekly_Sales+1), log.Size= log10 (data_joinedV$`Size (sq ft)`+1), log.prev_Weekly_Sales= log(data_joinedV$prev_Weekly_Sales+1), Temperature = data_joinedV$Temperature, log.Fuel_Price = log10 (data_joinedV$Fuel_Price), log.CPI = log10 (data_joinedV$CPI+1), Unemployment = data_joinedV$Unemployment, IsHoliday = data_joinedV$IsHoliday)
# pairs(data_joinedTlog)
```

```{r}
# Delete rowns with NA's
data_joinedTlog = na.omit(data_joinedTlog)
data_joinedTlog <- data_joinedTlog [!is.infinite(rowSums(data_joinedTlog)),]
summary(data_joinedTlog)

# Delete rowns with NA's
data_joinedVlog = na.omit(data_joinedVlog)
data_joinedVlog <- data_joinedVlog [!is.infinite(rowSums(data_joinedVlog)),]
summary(data_joinedVlog)
```



```{r}
fit <- lm(log.Weekly_Sales ~ log.Size + log.prev_Weekly_Sales + Temperature + log.Fuel_Price + log.CPI + Unemployment + IsHoliday , data= data_joinedTlog)
summary(fit)
```

### Extreme values(should be in identification of ptoblems and fixing the problems)

We tried to deal with the extreme values but, we realised that cleaning in such a big data set, would don't imporve that much the model.

### Multiple colliniarity

We need to check if among the independent or predictor variables there is any collinearity. For this, we will use thegeneral function `vif ()` which stands for Variance Inflation Factor. It determines which variables are more correlated with other variables in the system.

Correlated: none VIF=1, moderately 1<VIF<5, ** highly 5<VIF<10, ...

```{r}
# Variance Inflation Factor
vif(fit)
```

With this test we realise that the level of Vif of `Type` 

```{r}
# Refit the model (2) - drop Type
fit <- lm(Weekly_Sales ~ IsHoliday + `Size (sq ft)`+ prev_Weekly_Sales + Temperature + Fuel_Price + CPI + Unemployment, data= data_joinedT)
summary(fit)
```

The R quared went down because the variable Type probably counted twice in the system.

We will check the vif again.

```{r}
# Variance Inflation Factor
vif(fit)
```

And we can see now that all variables have a VIF around 1, what means that the correlation was between Type and Size. What claerly makes sense. `Type` and `Size` variable give very similar information (pressumably, size is based on type), although `Size` is much more precise and numerial. Consequeltly, We will drop the `Type` variable. 

### P-value of coefficients and R2/F-satitistics

Next we Will eliminate all the variables that don't contribute that much to the final formula. By elimitating the variables the model dint go any worse. It is a simpler model. R2 is insignificatly lower.

First we will drop CPI because the fit is a bit high and it has the lower 

```{r}
# Refit the model (3) - drop CPI
fit <- lm(log.Weekly_Sales ~ log.Size + log.prev_Weekly_Sales, data= data_joinedTlog)
summary(fit)
plot(fit)
```

```{r}
# Refit the model (3) - drop CPI
fit <- lm(log.Weekly_Sales ~ log.prev_Weekly_Sales, data= data_joinedTlog)
summary(fit)
plot(fit)
```


```{r}
# Refit the model (3) - drop CPI
#fit <- lm(Weekly_Sales ~ IsHoliday + `Size (sq ft)`+ prev_Weekly_Sales + Temperature + Fuel_Price + Unemployment, data= data_joinedT)
#summary(fit)
```
And next we will drop1 Temperature

<sup>1. This is somthing that has been analisexz while building the model but it was demonstrated that the R2 becamen lower and that p/-values of some variables too</sup>

```{r}
# Refit the model (4) - drop Temperature
fit <- lm(Weekly_Sales ~ IsHoliday + `Size (sq ft)`+ prev_Weekly_Sales + Fuel_Price + Unemployment, data= data_joinedT)
summary(fit)
```
Next we will drop IS Holiday

```{r}
# Refit the model (5) - drop Fuel PRice
fit <- lm(Weekly_Sales ~ IsHoliday + `Size (sq ft)`+ prev_Weekly_Sales + Unemployment, data= data_joinedT)
summary(fit)
```

Next we will drop Holidays

```{r}
# Refit the model (6) - IsHolidays
fit <- lm(Weekly_Sales ~ `Size (sq ft)`+ prev_Weekly_Sales  + Unemployment, data= data_joinedT)
summary(fit)
```

Next we will drop Unemployment

```{r}
# Refit the model (7) - drop Unemployment
fit <- lm(Weekly_Sales ~ Unemployment + prev_Weekly_Sales, data= data_joinedT)
summary(fit)
```

Next we will drop Size

```{r}
# Refit the model (8) - drop Size
fit <- lm(Weekly_Sales ~ `Size (sq ft)` + prev_Weekly_Sales, data= data_joinedT)
summary(fit)
```

## The final model

The final model takes as predictors: Prev_Weekly_Sales and Size.

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Refit the model (9) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ prev_Weekly_Sales + `Size (sq ft)`, data= data_joinedT)
summary(fit)
plot(fit, col = "grey")
```

```{r}
# Eliminate extreme values
cutoff <- 4/((nrow(data_joinedT)-length(fit$coefficients)-2)) 
# Cook's D plot, cutoff as 4/(n-k-1)
plot(fit, which=4, cook.levels=cutoff)                       
# identify D values > cutoff
plot(fit, which=5, cook.levels=cutoff)
data_joinedT <- data_joinedT[-which(rownames(data_joinedT)    
# Row names discovered in 2 rounds
    %in% c("44915", "196736", "150027")),]  
```

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ prev_Weekly_Sales + `Size (sq ft)`, data= data_joinedT)
summary(fit) #R2 = 96.67%
```

```{r}
# Eliminate extreme values
cutoff <- 4/((nrow(data_joinedT)-length(fit$coefficients)-2)) 
# Cook's D plot, cutoff as 4/(n-k-1)
plot(fit, which=4, cook.levels=cutoff)                       
# identify D values > cutoff
plot(fit, which=5, cook.levels=cutoff)
data_joinedT <- data_joinedT[-which(rownames(data_joinedT)    
# Row names discovered in 2 rounds
    %in% c("84228", "142641", "191011")),] 
```


```{r}
# Eliminate extreme values
cutoff <- 4/((nrow(data_joinedT)-length(fit$coefficients)-2)) 
# Cook's D plot, cutoff as 4/(n-k-1)
plot(fit, which=4, cook.levels=cutoff)                       
# identify D values > cutoff
plot(fit, which=5, cook.levels=cutoff)
data_joinedT <- data_joinedT[-which(rownames(data_joinedT)    
# Row names discovered in 2 rounds
    %in% c("84228", "142641", "191011")),] 
```

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ prev_Weekly_Sales + `Size (sq ft)`, data= data_joinedT)
summary(fit) #R2 = 96.67%
```

In the first plot We can see that the model is clearly liniar in the first image. We can see that there are some extreme values, that we can clean to see it it improves.

In the second one we can see that the have a heavy tailed we have a problem with the extrems. So, the observations should be alinegned with the line. Normal Q-Q plots that exhibit this behavior usually mean your data have more extreme values than would be expected if they truly came from a Normal distribution.

<br>


# log assessment

```{r}
# Find all predicted values for both the training set and the validation
data_joinedTlog$log.Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(data_joinedTlog, select=c(log.prev_Weekly_Sales, log.Size)))
data_joinedVlog$log.Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(data_joinedVlog, select=c(log.prev_Weekly_Sales, log.Size)))
```

```{r}

# Check how good is the model on the training set - correlation^2, RME and MAE
train.corr <- round(cor(data_joinedTlog$log.Pred.Weekly_Sales, data_joinedTlog$log.Weekly_Sales), 2)
train.RMSE <- round(sqrt(mean((10^ data_joinedTlog$log.Pred.Weekly_Sales -  10^ data_joinedTlog$log.Weekly_Sales)^2)))
train.MAE <- round(mean(abs(10^ data_joinedTlog$log.Pred.Weekly_Sales - 10^ data_joinedTlog$log.eekly_Sales)))
c(train.corr^2, train.RMSE, train.MAE)
```

```{r}
# Check how good is the model on the validation set - correlation^2, RME and MAE
valid.corr <- round(cor(data_joinedVlog$log.Pred.Weekly_Sales, data_joinedVlog$log.Weekly_Sales), 2)
valid.RMSE <- round(sqrt(mean((10^ data_joinedVlog$log.Pred.Weekly_Sales - 10^ data_joinedVlog$log.Weekly_Sales)^2)))
valid.MAE <- round(mean(abs(10^ data_joinedVlog$log.Pred.Weekly_Sales - 10^ data_joinedVlog$log.Weekly_Sales)))
c(valid.corr^2, valid.RMSE, valid.MAE)
```

# The model assessment

The last step of the process is to check the model to know if it is accurate or not. For the assessment we will use the validation sample of the data previously created.

```{r}
# Find all predicted values for both the training set and the validation
data_joinedT$Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(data_joinedT, select=c(prev_Weekly_Sales, `Size (sq ft)`)))
data_joinedV$Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(data_joinedV, select=c(prev_Weekly_Sales, `Size (sq ft)`)))
```

```{r}

# Check how good is the model on the training set - correlation^2, RME and MAE
train.corr <- round(cor(data_joinedT$Pred.Weekly_Sales, data_joinedT$Weekly_Sales), 2)
train.RMSE <- round(sqrt(mean((data_joinedT$Pred.Weekly_Sales -  data_joinedT$Weekly_Sales)^2)))
train.MAE <- round(mean(abs(data_joinedT$Pred.Weekly_Sales - data_joinedT$Weekly_Sales)))
c(train.corr^2, train.RMSE, train.MAE)
```

```{r}
# Check how good is the model on the validation set - correlation^2, RME and MAE
valid.corr <- round(cor(data_joinedV$Pred.Weekly_Sales, data_joinedV$Weekly_Sales), 2)
valid.RMSE <- round(sqrt(mean((data_joinedV$Pred.Weekly_Sales - data_joinedV$Weekly_Sales)^2)))
valid.MAE <- round(mean(abs(data_joinedV$Pred.Weekly_Sales - data_joinedV$Weekly_Sales)))
c(valid.corr^2, valid.RMSE, valid.MAE)
```

<br>

# Conclusions

This report describes, preprocesses and analyzes a set of internal and external data related to the sales of a minoist from the United States, in order to construct a precise multiple linear regression model for sales prediction.

The main conclusions reached with the interpretation of the final model are the following:

- From all the information available, the most powerful predictor of sales is the sales from the prior year.
- The second better was the size of the store.
- The influence of external factors is inexistent. 

Sales vary over time (for some reason not explained by the other predictors), and this variation is smooth - so one days sales are not too different from the day before or the day after.

In short, the model essentially says that in the absence of other information, sales should be the same as they were yesterday. 

Further work. The model could have been improved if the holidays and weeks were line up between years. Time constrain and the fact that all this were new to us, do this as further work.

The analysis of the final model with all the data normalised would have been also interesting.

# References