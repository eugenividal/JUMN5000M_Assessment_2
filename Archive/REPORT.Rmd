---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
title: "Understanding Data and its Environment: Report assessment"
author:
- name: Eugeni Vidal
keywords: "pandoc, r markdown, knitr"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
biblio-style: apsr
endnote: no
bibliography: My library.bib
---

# 1. Introduction
Forecasting or making predictions of sales is one of the most important and challenging issues for retailers around the world[@reference_needed].

This report explains the approach taken to forecast sales for a nationwide retailer in the U.S based on historical sales data for the departments of 45 stores. 

The consideration of the effects of promotional activities is an additional difficulty in the analysis given the fact that part of the promotion related data is absent from historical records. 

The whole process using R is described in the following sections below.

The report is reproducible based on code available at https://github.com/eugenividal/Understanding-data-report.

# 2. Data description

The first stage before describing the data is to load it into the R environment. To to that, we will use the `tidyverse` package and load it with the `library()`function:

install.packages("tidyverse")

```{r, message=FALSE}
library(tidyverse)
# load data
source("code/load-data.R")
```

We can check that these files have been loaded with the following command:

```{r}
ls()
```

We are provided with 4 data sets all in comma-separated values (csv) format: stores (45 obs. of 3 variables), features (8,190 obs. of 12 variables), train (421,570 obs. of 5 variables) and test (115,069 obs. of 5 variables). Each of the data sets and its features are explained below. Describe the type of each variable (categorial, numeric, etc...)

*	**stores.csv**

>- Store: the anonymised store number
>- Type: store type, A: supercentre, B: superstore, C: supermarket
>- Size : store size (in square feet)

*	**features.csv**

>- Store: the anonymised store number
>- Date:	the week with the dated Friday
>- Temperature: average temperature in the region
>- Fuel_Price: cost of fuel in the region
>- Promotions:	anonymised data related to promotions, mainly price reductions that the retailer is running. Promotion data is only available after Nov. 2011, and is not available for all stores all the time. Any missing value is marked with an NA.
>- CPI: the consumer price index
>- Unemployment: the unemployment rate
>- IsHoliday: whether the week is a special holiday week

*	**train.csv**

>- Store: the anonymised store number
>- Department: the anonymised department number
>- Date: the week with the dated Friday
>- Weekly_Sales: sales for the given department in the given store
>- IsHoliday: whether the week is a special holiday week

*	**test.csv**

The validation dataset have the same fields as the train.csv, except we need to predict the weekly sales for each triplet of store, department, and date from 02/11/2012 to 26/07/2013.

## 2.1 Quality

### Completeness

Promotion data is only available after November 2011 and is not available for all stores all the time, missing values are marked with an N/A. For promotion 1, 4157 of the 8191 values are marked with N/A, for promotion 2, 5269, for promotion 3, 4576 and for promotion 4, 4725 and for promotion 5 4139. Such a high volume of missing data could potentially have an impact on prediction if it is not dealt with in an appropriate way.

### Accuracy

There are some negative values within the data we have been given that we should be aware of. Within the features data set there are negative values for the temperature records and within the train data set there is 1286 negative values out of the 421571 data entries for the weekly sales figures.

### Consistency

The dates within the train, features and test datasets are all given in an English date format, despite them being from an American company. It is important to ensure that variables are formatted in the same way in each data set. 

The variable IsHoliday is given as a true or false statement which may be difficult to process further in to the analysis, so can potentially be changed to a binary. 

## 2.2 Relevance

At this stage of the process we are assuming that all variables will be relevant until the modelling process begins (see section 4). It is expected that variables such as the store size and type will have a constant effect, whereas temperature and the price of petrol may have more subtle implications and may only be seasonal fluctuations.

# 3. Data preparation

In this step we will carry out some "techniques" to arrange the data in a way that makes the analysis easier and to produce a better model.

## 3.1. Joining

First of all, we will join the three data sets (stores, train and features). This will allow us to make the analysis simpler and more straightforward. The data will be linked using the attributes in common. Store, to link stores with train dataset; and Store, Date and IsHoliday to link the first dataset joined and features dataset.

To performance the join we will use `dplyr` package and the type of join will be left (using the function `left_join ()`). In other words, we will merge the three datasets in one matching all the rows from the train dataset with the rest of the datasets.

No missing or duplicate key values were detected.

In conclusion, the data linkage process will not present great difficulties due to the fact that: there are common attributes to join the tables, all the tables have the same format (csv), and no missing or duplicate key values were found.


```{r}
library(dplyr)
# join store-level data onto training dataset (so we know size and type)
train_joined = left_join(train, y= stores)
# Join train_joined onto features dataset (so we know the ret of variables)
train_joined = left_join(train_joined, y= features)
```

The result is a data frame with all the features that contains 421,570 observations. IF we had done a full_joine there would have been 1,755 obsertacions that don't match between train and features. Why? the date is different - guess.


## 3.2. Cleaning

### Missing values

Use Pandasâ€™ interpolate function to estimate - or could not use these sections to train models (Chris).

Check how many NA values we have.

To know how many NA we have we can us the following code.

```{r}
# Count number of TRUEs
sum(is.na(train_joined))
# Find mising values
summary(train_joined)
# Fins index of missing values in Weekly_Sales column
which(is.na(train_joined$Weekly_Sales))
```

Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible.

## 3.3. Transformation

### Type conversion

str(train_joined$IsHoliday)
```{r}
# convert IsHoliday to numeric
train_joined$IsHoliday <- as.numeric(train_joined$IsHoliday)
```
class(train_joined$IsHoliday)

class(train$Date)

TO conver the infomration into dmy we will use the packe `lubridate`.

str(train_joined$Date)
```{r}
library(lubridate)
# convert date info in format 'dmy'
train_joined$Date <- dmy(train_joined$Date)
```
class(train_joined$Date)

The class change is done, but I couldn't change to the English format "%Y-%m-%d" to "%d/%m/%Y".

### Generating variables

We generate new columns: 1) Include a Week Number of the year (code needed); 2) Add a return column.

class(train$Date)

```{r}
# Week number of the year
train_joined$WeekNum <- as.numeric(format(train_joined$Date+3,"%U"))

```
I would like to have WeekNum at the beginning of rhe table. How can I reorder columns in a table?

## 3.4 Data reduction

Finally, we will reduce the data set? Shold we reduce the data set? It depends on the model we are going to use I guess. After performing this procedure we have x observations which makes our data more manageable for further analysis.

And this is the way the data set frame looks once pre-processed (show the data in case we reduce it).


## 3.5 Subseting the data

"We will partition the training set into two different data frames in order to keep our analysis consistent and avoid testing on our training data"

To do this we will use `caret` package.

install.packages("caret")
library(caret)

```{r}
# Subset the data into train and test
n = nrow(train_joined)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
train_joined.train = train_joined[trainIndex ,]
train_joined.validation = train_joined[-trainIndex ,]
```

# 4. Exploratory analysis
"Once the data has been input and clean up, the next step is to explore each of the variables one at a time. This will provide us with inforamtion about the distribution of each varible, which is helpful in understanding the characteristics of the sample, identifying unexpected or problematic avalues, and selecting appropiate statistical methods". NExt a subset of variables is typically studied two at a time. This step can help to uncover basic relationships among variables, and is a useful first step in developing more complex models".

"For a small glimpse of how the data looks like we can refer to the following picture".

```{r}
head(train_joined.train)

```

```{r}
aggregate(train_joined.train[,"Weekly_Sales"], by=train_joined.train[,c("Store"), drop=FALSE], mean)
```
Graphs data visualization with `ggplot` package:



```{R}
library(ggplot2)
# Plot sales per each of the department
ggplot(train_joined,aes(x= Date, y= Weekly_Sales)) + geom_point(aes(color=Dept))+ labs(title = "Sales per store department", x = "Date", y = "Sales", color = "Department")

```

```{R}
# Plot sales per type of store
ggplot(train_joined,aes(x= Date, y= Weekly_Sales)) + geom_point(aes(color=Type))+ labs(title = "Sales per store department", x = "Date", y = "Sales", color = "Type of store")

```

Graphs to see the correlation between the different variables

There are many packages and approach for forecasting.
We could use the `lm()` function to do a linear regression, for example.
Here we use the xgboost package.

class(test$Date)

```{R}
# Plot weekly sales vs CPI
ggplot(train_joined,aes(x= CPI, y= Weekly_Sales)) + geom_point(aes(color=Type)) +stat_summary(fun.data=mean_cl_normal) + 
  geom_smooth(method='lm') + labs(title = "weekly sales vs CPI", x = "Date", y = "Sales", color = "Type of store")
```


```{r}
# Plot weekly sales vs Unemployment
ggplot(train_joined,aes(x= Unemployment, y= Weekly_Sales)) + geom_point(aes(color=train_joined$Type))+stat_summary(fun.data=mean_cl_normal) + 
  geom_smooth(method='lm')
```

```{R}
# Plot weekly sales vs Temperature
ggplot(train_joined,aes(x= Temperature, y= Weekly_Sales)) + geom_point(aes(color=train_joined$Type)) + geom_smooth()
```
 
There is no clear correlation between the varibles previously graphed. BUt there is correlation between...

Correlation matrix between all of our numerical features?

# 5. Creating predictive models
 
```{r}
m1 = lm(Weekly_Sales ~ Dept + Store + Type + Promotion1 + Promotion2 + Promotion3 + Promotion4 + Promotion5 + CPI + Unemployment, data = train_joined)
summary (m1)

```

Linear model to find a specific value for Weekly Sales that we want to predict?

# 6. Evaluation of forecasting accuracy 

table(features$Date)

# 7. Conclusions

You should, in the conclusions, report on the limitations of the data you have used or what future studies of the same topic might need to look for.

# References

