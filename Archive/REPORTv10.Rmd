---
title: 'Understanding Data and its Environment: Report assessment'
author: "Eugeni Vidal"
date: "March 15, 2018"
output:
  pdf_document: 
    fig_caption: yes
    number_sections: yes
    toc: yes
  word_document:
    toc: yes
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
bibliography: My library.bib
---

\pagebreak

```{r,echo=FALSE,message=FALSE,warning=FALSE}
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

# Introduction

The cambridge Dictionary describes sales forecasting as "the statement of what the amount or value of a company's sales is likely to be in the future, based on information available now about the market, past sales, etc"[@cambridge_dictionary_cambridge_nodate].

The increase of competition, complexity in business tasks, and the fact that nowadays circumstances, in general, tend to change more rapidly makes increasingly important and necessary for companies the use of forecasting technics for the prediction of their future prospects [@lancaster_forecasting_1985, p1].

<Aim>
This report aims to describe, pre-process and analyse a set of data based on historical sales data collected from a nationwide retailer in the U.S as well as on external factors, so as to lead to the development of an accurate predictive model. 

<Method>
The methodology followed is divided into 7 different steps, from describing the data to building and assessing the model developed. Notice that although each step is taken in order, the whole process has to be understood as a set of nested loops rather than a straight line (see figure 1).

![Methodology diagram \label{}](images/Methodology diagram.jpg)

<br>

The whole process has been done using the open software R and it is reproducible based on code available at https://github.com/eugenividal/Understanding-data-report.

This report is written as clearly and easily as possible, with the pretense that any person, without much prior knowledge in forecasting or in the R software, can understand it. For this purpose, the document describes not only the statistical process followed, but also the code used with the software R. 

<br>

# Data description

The first thing to start a project with the R software is to install the packages that we will need and load their libraries. To do this we use the code shown below. 

```{r,  message=FALSE, cache=FALSE}
# Activate libraries
library(tidyverse)
library(VIM)
library(dplyr)
library(psych)
library(lubridate)
library(caret)
library(car)
```

Secondly, we will load the data into the R environment.

```{r, message=FALSE, cache=FALSE}
# Load data
stores = read_csv("data/stores.csv")
features = read_csv("data/features.csv")
test = read_csv("data/test.csv")
train = read_csv("data/train.csv")
```

<br>

We are provided with 4 data sets (stores, features, train, and test). All of them with the same format: comma-separated values (csv). 

Below is a brief description of each of the datasets and their variables:

<Describe the type of each variable (categorical, numeric, etc)?>

**stores.csv (45 obs. of 3 variables)**

- Store: the anonymised store number <numeric> 
- Type: tore type, A: supercentre, B: superstore, C: supermarket <character>
- Size (sq ft): store size (in square feet) <numeric>

**features.csv (8,190 obs. of 12 variables)**

- Store: the anonymised store number <numeric>
- Date:	the week with the dated Friday <character>
- Temperature: average temperature in the region <numeric>
- Fuel_Price: cost of fuel in the region <numeric>
- Promotions:	anonymised data related to promotions, mainly price reductions that the retailer is running <numeric>
- CPI: the consumer price index <numeric>
- Unemployment: the unemployment rate <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**train.csv (421,570 obs. of 5 variables)**

- Store: the anonymised store number <numeric>
- Department: the anonymised department number <numeric>
- Date: the week with the dated Friday <character>
- Weekly_Sales: sales for the given department in the given store <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**test.csv (115,069 obs. of 5 variables)**

The validation dataset has the same fields as the train.csv, except we need to predict the weekly sales for each triplet of store, department, and date from 02/11/2012 to 26/07/2013.

<br>

Variables of each dataset have been explored through counts, summary statistics, crosstabulation and visualisations in order to identify potential inconsistences or problems in the data.

The main problems detected are briefly described below.

**1. Inconsistent data encoding**. In the stores dataset, some stores might be wrongly classified. Two stores under 50,000 sq ft are coded as type B and other two as type A, when presumably small size stores (<50000 sq ft) should be type C. 

```{r, include=FALSE, cache=FALSE}
# Crosstab Size and Type store data set
crosstab_Size_Type <- table(stores$Type, stores$`Size (sq ft)`)
# Print first 10 rows of mydata
head(crosstab_Size_Type, n=10)
```

**2. Missing values**. In the features dataset, 24,040 values are missing, almost 50% in the promotion variables and 7% in the `CPI` and `Unemployment` ones. With the function of the VIM package aggr() we can visualise them for each variable alone and for each combination of variables.

```{r , fig.cap=c("Missing values"), fig.height=3.5, fig.width=5.5, echo=TRUE}
# Plot missing values
aggr(features, prop=FALSE, col = "grey", cex.lab = 0.85, cex.axis = 0.65, cex.main = 0.85)
```

There are not missing values in the ret of the datasets.

**3. Negative values**. Some of the values in the promotions variables are negative: 4 in `Promotion1`, 25 in `Promotion2`, 13 in `Promotion3`, and 2 in `Promotion5`. Also within the `Weekly Sales` (1286 of 421571 (0.3%)) variable in the train dataset.

**4. Class type errors**. The  class of the `Date` variable within the train, features and test datasets is character instead of date. This doesn't allow to sort the data properly. Also in terms of class, it would be useful to have the variable IsHoliday in numeric type (1 or 0) instead of boolean (true or false). 

**5. Data not normally distributed**. Finally, to mentione that the data in some variables is not normally distributed. Weekly Sales data is clearly left-skewed as can be seen in the histogram below. But also Size have a not normal distribution, rather comb, and Fuel Price and CPI a bimodal. 

```{r,include=FALSE, cache=FALSE}
# Scientific notation Off
options(scipen=999)
```

```{r}
features$Store <- NULL
features$Date <- NULL
```


```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Plot a histogram

hist(train$Weekly_Sales, cex.lab = 0.85, cex.axis = 0.85, cex.main = 0.85, col = "grey", 
main=NULL, xlab="Weekly Sales", ylab="Frequency")
```

<br>

<Extreme values?. Here, in fixing problems or fiting the model?>

```{r,include=FALSE, cache=FALSE}
# Scientific notation On
options(scipen=001)
```

<br>

# Data joining

The next step is to join the stores, train and features datasets in a single one. This will allow us to make the analysis simpler and more straightforward. 

The test dataset will be left aside to gauge the effectiveness of the models once build.

To performance the join, we apply a left join. We chose this type of join because we want to end up having the same number of rows as the train dataset which contains the variable we want to predict `Weekly Sales`.  

First, we link the stores dataset with the train one, using the common attribute `Store`; then the resultant dataset with the features dataset, using the common attributes `Store`, `Date` and `IsHoliday`. See the code below.

```{r}
# Join store-level data onto training dataset (so we know size and type)
data_joined = left_join(train, y= stores)
# Join train_joined onto features dataset (so we know the rest of variables)
data_joined = left_join(data_joined, y= features)
```

The result is a data frame that contains 421,570 observations of 16 variables. Notice that 1,755 observations don't match between train and features. This is because we did a left join and the features datasets collect data until 2013-07-26 while the train dataset only until 2013-07-26. This way, we leave behind 1,755 observations and the 585 NA's previously detected in `CPI`, `Unemployment` and `IsHoliday`.

<br>

# Cleaning and fixing problems with the data

In this section, we will clean and fix inconsistencies or potential problems identified and already mentioned in the section data description. 

## Resolving inconsistent data encoding

We will assume that the type of store is based on size and consequently, will recode all stores under 50000 sq ft as C type. This way we solve the potential inconsistency of 4 stores under 5000 sq ft coded as type A and B.

```{r}
# Recode `stores$Type` based on `stores$Size (sq ft)`
data_joined$Type[(data_joined$`Size (sq ft)`<50000)] <- "C"
data_joined$Type[(data_joined$`Size (sq ft)`>=50000) & (data_joined$`Size (sq ft)`
<150000)] <- "B"
data_joined$Type[(data_joined$`Size (sq ft)`>=150000)] <- "A"
```

## Dealing with missing values

There are several approaches when we find missing data: delete the cases containing missing data, or replace (impute) the missing values with reasonable alternative data values [@kabacoff_r_2011, p353].

Deciding how to treat missing values will depend on your estimation of which procedures will produce the most reliable and accurate results [@kabacoff_r_2011, p.354].

"There is no established cutoff from the literature regarding an acceptable percentage of missing data in a data set for valid statistical inferences" [@dong_principled_2013]. Schafer [-@schafer_multiple_1999] argues
that a missing rate of 5% or less is inconsequential, while Bennett [-@bennett_how_2001] considers that more than 10% of values missing is likely to  biased the statistical analysis.

Given that the percentage of missing values in promotions is around 50%, we consider wil will apply listwise deletion, in other words, we will not consider them in the statistical analysis and will delete them from the dataset.

```{r}
# Delete promotions
data_joined$Promotion1 <- NULL
data_joined$Promotion2 <- NULL
data_joined$Promotion3 <- NULL
data_joined$Promotion4 <- NULL
data_joined$Promotion5 <- NULL
```

The missing values identified in the `CPI` and `Unemployment` variables disappeared when the datasets were merged. With the left_join, mentioned before, the observations that did not match with the train data set were delated and among them were the NAs identified in these variables.

## Interpretation of negative values

After analyzing the negative values of the `Weekly_Sales` variable we have concluded that they are returned products from previous weeks. So, no changes will be done in this sense. 

## Data type conversion

In order to make the variable `IsHoliday` more manageable, we will convert the data type into numeric using the general function `as.numeric()`.

```{r}
# Convert IsHoliday to numeric
data_joined$IsHoliday <- as.numeric(data_joined$IsHoliday)
```

We will also convert the `Date` class into a more appropiate format  such as date. To do that, we will use the `mdy ()` function of the lubridate package.

```{r message=FALSE}
# Convert date info to date format 'dmy'
data_joined$Date <- dmy(data_joined$Date)
```

## Consideration of data normalisation

It might be useful to have the errors normally distributed with constant variance in order to produce prediction intervals, although it is not considered necessary for forecasting [@hyndman_forecasting:_2017]. So, at this stage at least, we will keep all the variables with their original distribution. It could be preferable since the model would be more understandable and interpretable. 
<br>

# Data transformation

Once the inconsistencies or potential problems are fixed, we will make some transformations in order to prepare the dataset for building the  model.

First, we will create a week number of year in order to compare them.

```{r}
# Create a week number of the year variable
data_joined$WeekNum <- as.numeric(format(data_joined$Date+3,"%U"))
```

Secondly, We will create a year variable.

```{r}
# Create a year variable
data_joined$year = lubridate::year(data_joined$Date)
```

Thirdly, we will create a variable of previous years. The previous year's sales can be one of the most powerful predictors in sales forecasting [@zoltners_complete_2001]. 

```{r, echo=TRUE, fig.cap=c("Correlation Weekly Sales ~ Weekly Sales"), fig.height=3.5, fig.width=5.5}
# Create a previous year sales variable
prevyear = select(data_joined, year, WeekNum, Dept, Store, prev_Weekly_Sales = Weekly_Sales)
prevyear$year = prevyear$year + 1
summary(prevyear$year)
# Create variable Sales previous year
data_joined = left_join(data_joined, prevyear)
```

As we can see in the graph, Weekly Sales of the given year are highly correlated with the variable sales of the previous year created. 

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Plot correlation Weekely Sales ~ Previous Year Weekly Sales
plot(data_joined$Weekly_Sales, data_joined$prev_Weekly_Sales,col = "grey", main=NULL, 
     xlab="Weekly Sales", ylab="Previous year Weekly Sales")
```

<br>

Due to the fact that the new variable is based on `Weekly_Sales` of the previous year, there will be one year of missing values. 

We will handle with this applying again listwise deletion. This will reduce the sample size by 38% (from 421,570 to 261,541) and consequently could reduce statistical power of our model dataset. However, an approach with the entire dataset (including the cases with missing data) could bias the results of the subsequent analysis [@bennett_how_2001, p.464].

```{r}
# Delete rowns with NA's
data_joined = na.omit(data_joined)
```

<The next step is to line-up holidays from one year and the other.>

```{r,include=FALSE, cache=FALSE}
# Match holiday weeks to holiday weeks. Adjust for Easter and adjust for the different Christmas week 

```

```{r,include=FALSE, cache=FALSE}
# Because of week mismatches between years take average of previous years sales 

```

Finally, we will rearrange the dataset in the most logical and comfortable way for us to build the model.

```{r, include=FALSE, cache=FALSE}
# Reorganise the columns
data_joined <- data_joined[c(2,1,6,7,3,4,14,8,9,10,11,12,5)]
```

Using the `head()` function we can have a glimpse of how the final model dataset looks like.

```{r}
# Glance at the first 6 rows of the model dataset
head(data_joined)
```
<br>

# Data partition

In this section, the model dataset is divided into two parts. The first one, the training set (data_joinedT), will be used to build the model; and the second one, the validation set (data_joinedV), to adjust it. 

<The test set that we put aside before, will be used to gauge the effectiveness of the models when applied to unseen data>. 

<why 80%? (reference needed)>

For the partition, we will use the `caret` package.

```{r message=FALSE}
#subset the data into train and test
n = nrow(data_joined)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
data_joinedT = data_joined[trainIndex ,]
data_joinedV = data_joined[-trainIndex ,]
```

<br>

# Building the model

## The choice of technique

The choice of forecasting technique is a major consideration. There are many different methods to apply, from pure guesswork to highly complex mathematical analysis [@lancaster_forecasting_1985, p.15].

Three factors are determinant on the decision: accuracy, time-scale and cost [@lancaster_forecasting_1985, p.37-38].

In this case, we will opt for a multiple simple regression apporach and this is for three fundamental reasons:

- Although it is debatable if complex models might be more precise, multiregresion regression clearly simpler, easier and more understandable. 

- Multiregression regression is quicker.

- Multiregression is also cheaper.

Multiple regression analysis is described as "a multivariate statistical technique for examining the linear correlations between two or more independent variables and a single dependent variable"[@lu_neural_2011, p.119] .

The general form of multiple regression is:

<center><span style="font-size:larger;">*y~i~ = B~0~ + B~1~x~1~,~i~ + B~2~x~2~,~i~+...+ B~k~x~k~,~i~ + e~i~,*</span></center>

Where,

- <span style="font-size:larger;">*y~i~*</span></center> is the dependent variable (or variable to be forecast)
- <span style="font-size:larger;">*x~i~,~i~...x~k~,~i~*</span></center> are the independent variables (or predictors)
- <span style="font-size:larger;">*B~0~*</span></center> is the y-intercept
- <span style="font-size:larger;">*B~1~,...B~k~*</span></center> are the coefficients that measure the marginal effects of the predictors
- <span style="font-size:larger;">*e*</span></center> are the residuals (a random variable that captures the fact that regression models typically do not fit the data perfectly).

In our model, Weekly Sales would be the dependent variables and the rest of variables the predictors or independent variables.

## The full model

<First of all we will examine the relationships among the variables two at a time. WE can see the the bivariate correlations using the cor() function, and generate a the scatterplotMatrix() function in the car package.>

```{r}
# Remove negative values
data_joinedT [data_joinedT < 0] <- NA
```

```{r,include=FALSE, cache=FALSE}
# Select a subset of variables for regression modelling
data_joinedT <- subset(data_joinedT, select = c(`Size (sq ft)`, Weekly_Sales, prev_Weekly_Sales))
```


```{r}
pairs(data_joinedT)
```

```{r}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ prev_Weekly_Sales + `Size (sq ft)`, data= data_joinedT)
summary(fit) #R2 = 96.67%
plot(fit, col = "grey")
```

```{r}
data_joinedTlog <- data.frame(log.Weekly_Sales = log (data_joinedT$Weekly_Sales+1), log.Size= log (data_joinedT$`Size (sq ft)`+1), log.prev_Weekly_Sales= log(data_joinedT$prev_Weekly_Sales+1))
pairs(data_joinedTlog)
```

```{r}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(log.Weekly_Sales ~ log.prev_Weekly_Sales + log.Size, data= data_joinedTlog)
summary(fit) #R2 = 96.67%
plot(fit, col = "grey")
```


```{r}
# Bivariate correlations
# cor(data_joinedT)
# scatterplot matrix
# scatterplotMatrix(data_joinedT, spread=FALSE, lty.smooth=2,
# main="Scatter Plot Matrix")
```

```{r,include=FALSE, cache=FALSE}
# Scientific notation Off
options(scipen=999)
```

The next step is to fit the full model with the basic function `lm ()` and see the output. 

```{r}
# Fit the model (1)
fit <- lm(Weekly_Sales ~ + IsHoliday + Type + `Size (sq ft)`+ prev_Weekly_Sales + Temperature + Fuel_Price + CPI + Unemployment, data= data_joinedT)
summary(fit) #R2 = 96.71%
```

Regression coefficients represent the mean change in the response variable for one unit of change in the predictor variable while holding other predictors in the model constant.

A predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable. 

And thee start represent that the probability rejecting this values is low. So we shold't reject at this point any of the candidate variables.

The R2 explain how well the model explains the variation in the data.This model explains the 96.71% of errors or residuals from the linear model.

Taken together, the predictor variables account for 97 percent of the variance in Weekly Sales.

## Selecting predictors

Once analysed the full model, We perform additional tests and use a stepwise selection of variables by backwards elimination. Fitting different reduced models and keeping those variables more suitable.

First we will check for multi-collinearity with Variance Inflation Factor.

Up to this point, we’ve assumed that the predictor variables don’t interact. TO chec if they do we can. 

Correlated: none VIF=1, moderately 1<VIF<5, ** highly 5<VIF<10, ...

```{r}
# Variance Inflation Factor
vif(fit)
```

Whit this test we realise that the level of Vif of `Type` is high. 

`Type` and `Size` variable give very similar information (pressumably, size is based on type), although `Size` is much more precise and numerial. Consequeltly, We will drop the `Type` variable.

```{r}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~. - Type, data= data_joinedT)
summary(fit) #R2 = 96.69%
```

```{r}
# Variance Inflation Factor
vif(fit)
```

VIF, F-ratio and p-values say it is good, so no need to do anything else.

The bext is to check the Monoscedasticity among them (even distribution of residuals)

Finally the p-value of coefficients and R2?F statistic of the model.

We elimitated all the variables that don't contribute that much to the final formula. By elimitating the variables the model dint go any worse. It is a simpler model. R2 is insignificatly lower.

## Final model and interpretation

The final model takes as predictors: Prev_Weekly_Sales, Size and .

```{r, echo=TRUE, fig.height=3.5, fig.width=5.5, fig.cap=c("Weekly Sales histogram")}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ prev_Weekly_Sales + `Size (sq ft)`, data= data_joinedT)
summary(fit) #R2 = 96.67%
plot(fit, col = "grey")
```

In the first plot We can see that the model is clearly liniar in the first image. We can see that there are some extreme values, that we can clean to see it it improves.

In the second one we can see that the have a heavy tailed we have a problem with the extrems. So, the observations should be alinegned with the line. Normal Q-Q plots that exhibit this behavior usually mean your data have more extreme values than would be expected if they truly came from a Normal distribution.

Observe that R-squared almost did not change and all Ps are good.

```{r}
# Refit the model (3) - Size
fit <- lm(Weekly_Sales ~ prev_Weekly_Sales, data= data_joinedT)
summary(fit) #R2 = 96.67%
```

Observe that R-squared almost did not change and all Ps are good.

<br>

# The model assessment

Finally, to check the model to know if this is a good or not a good model we will use the validation sample of the data.

```{r}
#     Find all predicted values for both a training set and a validation set
data_joinedT$Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(data_joinedT, select=c(prev_Weekly_Sales, `Size (sq ft)`)))
data_joinedV$Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(data_joinedV, select=c(prev_Weekly_Sales, `Size (sq ft)`)))
```

```{r}
# The theoretical model performance is defined here as R-Squared
summary(fit)
```

```{r}

# Check how good is the model on the training set - correlation^2, RME and MAE
train.corr <- round(cor(data_joinedT$Pred.Weekly_Sales, data_joinedT$Weekly_Sales), 2)
train.RMSE <- round(sqrt(mean((data_joinedT$Pred.Weekly_Sales -  data_joinedT$Weekly_Sales)^2)))
train.MAE <- round(mean(abs(data_joinedT$Pred.Weekly_Sales - data_joinedT$Weekly_Sales)))
c(train.corr^2, train.RMSE, train.MAE)
```

```{r}
# Check how good is the model on the validation set - correlation^2, RME and MAE
valid.corr <- round(cor(data_joinedV$Pred.Weekly_Sales, data_joinedV$Weekly_Sales), 2)
valid.RMSE <- round(sqrt(mean((data_joinedV$Pred.Weekly_Sales - data_joinedV$Weekly_Sales)^2)))
valid.MAE <- round(mean(abs(data_joinedV$Pred.Weekly_Sales - data_joinedV$Weekly_Sales)))
c(valid.corr^2, valid.RMSE, valid.MAE)
```

<br>

# Conclusions

This report develops an accurate model for the prediction os sales for a nationwide retailer in the U.S. 

<summary of all the steps followed>
After 7 different steps, from describing the data to building and assessing the model developed, the conclusion and interpretations of it are the following:

<conclusive points>

- From all the information that we had available, the best predictor of is the sales from the prior year. it has the most predictive power
- The second better was the size of the store.
- The influence of external factors is minimal. 

Sales vary over time (for some reason not explained by the other predictors), and that this variation is smooth - so one days sales are not too different from the day before or the day after.

This model essentially says that in the absence of other information, sales should be the same as they were yesterday. 

This model could have been improved if the holidays and week were line up between years. The time constrain and the fact that all this were new to us, do this as further work.

We could have also deal with the normalization of the variables which were not normaly distributed.

<Las statement>
With this model the organisation is able to plan bussness in future. Probably more accurate model could have been done, but the strong point of this one is that is simpler and more understandable for non very matemathical people.

# References