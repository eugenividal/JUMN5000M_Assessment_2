---
title: "Understanding Data and its Environment: Report assessment"
author: "Eugeni Vidal"
date: March 15, 2018
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
  pdf_document: default
  word_document: default
  slidy_presentation:
bibliography: references.bib
---

# 1. Introduction

The cambridge Dictionary describes sales forecasting as "the statement of what the amount or value of a company's sales is likely to be in the future, based on information available now about the market, past sales, etc".

The increase of competition, complexity in business tasks, and the fact that nowadays circumstances, in general, tend to change more rapidly makes increasingly important and necessary for companies the use of forecasting technics for the prediction of their future prospects (Forecasting for sales p.1).

<Aim>
This report aims to describe, pre-process and analyse a set of data based on historical sales data collected from a nationwide retailer in the U.S as well as on external factors, so as to lead to the development of an accurate predictive model. 

<Method>
The methodology followed is divided into 7 different steps, from describing the data to building and assessing the model developed. Each step is taken in order, although the whole process has to be understood as a set of nested loops rather than a straight line (see figure 1).

![**Figure 1. Methodology diagram**](images/Methodology diagram.jpg)

The whole process has been done using the open software R and it is reproducible based on code available at https://github.com/eugenividal/Understanding-data-report.

This report is written with the pretension that any person, without much prior knowledge in forecasting or in the software R, can understand the process clearly, update the model or if it is considered appropriate to improve it. For this purpose, the paper describes not only the statistical process followed, but also the code used with the software R.


# 2. Step 1. Defining the data

```{r, include=FALSE, cache=FALSE}
# install.packages("tidyverse")
# install.packages("dplyr")
# install.packages("psych")
# install.packages("lubridate")
# install.packages("caret")
# install.packages("car")

library(tidyverse)
library(dplyr)
library(psych)
library(lubridate)
library(caret)
library(car)
```

The first stage before describing the data is to load it into the R environment. To do that, we will use the `tidyverse` package and load it with the `library()`function:

```{r, message=FALSE, cache=FALSE}
library(tidyverse)
# Load data
stores = read_csv("data/stores.csv")
features = read_csv("data/features.csv")
test = read_csv("data/test.csv")
train = read_csv("data/train.csv")
```
<br>
We are provided with 4 data sets (stores, features, train, and test). All of them with the same format: comma-separated values (csv). 

<Describe the type of each variable (categorical, numeric, etc)?>

**stores.csv (45 obs. of 3 variables)**

- Store: the anonymised store number <numeric> 
- Type: tore type, A: supercentre, B: superstore, C: supermarket <character>
- Size: store size (in square feet) <numeric>

**features.csv (8,190 obs. of 12 variables)**

- Store: the anonymised store number <numeric>
- Date:	the week with the dated Friday <character>
- Temperature: average temperature in the region <numeric>
- Fuel_Price: cost of fuel in the region <numeric>
- Promotions:	anonymised data related to promotions, mainly price reductions that the retailer is running <numeric>
- CPI: the consumer price index <numeric>
- Unemployment: the unemployment rate <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**train.csv (421,570 obs. of 5 variables)**

- Store: the anonymised store number <numeric>
- Department: the anonymised department number <numeric>
- Date: the week with the dated Friday <character>
- Weekly_Sales: sales for the given department in the given store <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**test.csv (115,069 obs. of 5 variables)**

The validation dataset has the same fields as the train.csv, except we need to predict the weekly sales for each triplet of store, department, and date from 02/11/2012 to 26/07/2013.

<br>

After loading the data, variables of each dataset have been explored through counts, summary statistics, crosstabulation and visualisations.

This exercise of data exploration is fundamental for the usefulness of later analysis (Berry, 2004, p.65). All these problems could potentially have an impact on prediction if we do not deal with them in an appropriate way (see section 4).

The potential inconsistences or problems identified are the following:

1. In the stores dataset, some stores might be wrongly classified. Two stores under 50,000 sq ft are coded as type B and other two as type A, when presumably small size stores (<50000 sq ft) should be type C. 

```{r, include=FALSE, cache=FALSE}
# crosstab Size and Type store data set
crosstab_Size_Type <- table(stores$`Size (sq ft)`, stores$Type)
# print first 10 rows of mydata
head(crosstab_Size_Type, n=10)
```

2. In the features dataset, 24040 values are missing, almost 50% in the promotion variables and 7% in the `CPI`, `Unemployment` and `IsHoliday`ones. To know how many NA we have and in which variables we use the code below. With a summary we can see how those missing values are distributed.

```{r}
# Count number of TRUEs
sum(is.na(features))
# Find missing values
summary(features)
```

3. Some of the values in the promotions variables are negative: 4 in `Promotion1`, 25 in `Promotion2`, 13 in `Promotion3`, and 2 in `Promotion5`. Also within the `Weekly Sales` (1286 of 421571 (0.3%)) variable in the train dataset.

4. The  class of the `Date` variable within the train, features and test datasets is character instead of date. This doesn't allow to sort the data properly. Also in terms of class, it would be useful to have the variable IsHoliday in numeric type (1 or 0) instead of boolean (true or false). 

5. Finally, to mentione it has been detected that the data in some variables is not normallly distributed. The following histogram of ‘Weekly Sales‘, created with the generic function hist (), is cleare example of that.

```{r,include=FALSE, cache=FALSE}
# Count number of TRUEs
options(scipen=999)
```

```{r}
# Count number of TRUEs
options(scipen=999)
hist(train$Weekly_Sales)
options(scipen=001)
```

```{r,include=FALSE, cache=FALSE}
# Count number of TRUEs
options(scipen=999)
```

# 3. Step 2. Joining the model data set

The next step is to join the stores, train and features datasets in a single one. The test dataset will be left aside to gauge the effectiveness of the models once build.

To performance the join, we use the `dplyr` package and the type of join applied is left. We chose a left join because we want to end up having the same number of rows as the train dataset which contains the variable we want to predict `Weekly Sales`.  

First, we link the stores dataset with the train one, using the common attribute `Store`; then the resultant dataset with the features dataset, using the common attributes `Store`, `Date` and `IsHoliday`. See the code below.

```{r}
library(dplyr)
# Join store-level data onto training dataset (so we know size and type)
train_joined = left_join(train, y= stores)
# Join train_joined onto features dataset (so we know the rest of variables)
train_joined = left_join(train_joined, y= features)
```

The result is a data frame that contains 421,570 observations of 16 variables. Notice that 1,755 observations don't match between train and features. This is because we did a left join and the features datasets collect data until 2013-07-26 while the train dataset only until 2013-07-26. This way, we leave behind 1,755 observations and the 585 NA's previously detected in `CPI`, `Unemployment` and `IsHoliday`.

# 4. Step 3. Cleaning and fixing problems with the data

In this section, we will fix inconsistencies or potential problems identified in the section data description. 

<"What is or isn’t a problem varies with the data mining technique used"" (Berry, 2004, p.72)>. 

### 4.1. Inconsistent Data Encoding

We will assume that the type of store is based on size and consequently, will recode store As and Bs under 50000 sq ft as C type. 


```{r}
# Recode stores type based on size (some errors detected)
train_joined$Type[(train_joined$`Size (sq ft)`<50000)] <- "C"
train_joined$Type[(train_joined$`Size (sq ft)`>=50000) & (train_joined$`Size (sq ft)`<150000)] <- "B"
train_joined$Type[(train_joined$`Size (sq ft)`>=150000)] <- "A"
```


### 4.2. Missing values

Given that promotion variables have almost 50% of their values missing (N.A), we will not consider and remove them from the training   dataset<(reference needed)>.

```{r}
# Delete promotions because 50% data are missing
train_joined$Promotion1 <- NULL
train_joined$Promotion2 <- NULL
train_joined$Promotion3 <- NULL
train_joined$Promotion4 <- NULL
train_joined$Promotion5 <- NULL
```

### 4.3. Negative values

The negative values in `Weekly Sales` could be attributed to mitakes taping. We convert the negative in postive values. <(reference needed)>. 

NEGATIVE VALUES ARE RETURNS (CHECK IF THEY ARE AFTER BIG SELLS DAYS)
```{r}
# Remove negative values, although  they can be returns (to apply the log)
# train_joined <- train_joined[train_joined$Weekly_Sales > 0, ]
```

```{r}
# Convert IsHoliday to numeric
train_joined$IsHoliday <- as.numeric(train_joined$IsHoliday)
```

```{r}
# Convert date info in format 'dmy'
train_joined$Date <- dmy(train_joined$Date)
```

### 4.4. Normality of distribution

Check for non linearity (visually) and transform variables

```{r}
# Plot a correlation plot
# pairs.panels(train_joined, col="red")
```

```{r}
# Log transformation of Weekly sales to have a more normal distribution
#train_joined.sel$Weekly_Sales <- log10(train_joined.sel$Weekly_Sales)
# train_joined.sel$old_sales <- log10(train_joined.sel$old_sales)
```

### 4.5. Check extreme values

Not many founded and there are so many observation that take some of is insignificant.

### 4.6 Impractical class type

We will change the class type of `IsHoliday` variable into numeric using the general function `as.numeric()`.

```{r}
# convert IsHoliday to numeric
train_joined$IsHoliday <- as.numeric(train_joined$IsHoliday)
```

To convert the date information into date we will use the package `lubridate` and the function `mdy ()`.

```{r message=FALSE}
library(lubridate)
# convert date info in format 'dmy'
train_joined$Date <- dmy(train_joined$Date)
```


# 5. Step 4. Transforming Variables

```{r}
# Create a week number of the year
train_joined$WeekNum <- as.numeric(format(train_joined$Date+3,"%U"))
train_joined$year = lubridate::year(train_joined$Date)
```

```{r}
# Create a previous year sales variable
prevyear = select(train_joined, year, WeekNum, Dept, Store, old_sales = Weekly_Sales)
prevyear$year = prevyear$year + 1
summary(prevyear$year)
# Create variable Sales previous year
train_oldsales = left_join(train_joined, prevyear)
```

IF I OMMIT THE MISSING VALUES IS 1 year less

```{r}
# replace nas for zeros
# train_oldsales$old_sales[is.na(train_oldsales$old_sales)] <- 0

# clean rowns with na's
train_oldsales = na.omit(train_oldsales)
```

```{r}
# Match holiday weeks to holiday weeks. Adjust for Easter and adjust for the different Christmas week 

```

```{r}
# Take logical average for missing data 

```

```{r}
# Because of week mismatches between years take average of previous years sales 

```

```{r, include=FALSE, cache=FALSE}
# Reorganise the columns
train_joined <- train_oldsales[c(2,1,6,3,4,14,5,7,8,9,10,11)]
```

```{r}
# Glance at the first 6 rows of the model dataset
head(train_joined)
```

# 6. Step 5. Data partition

```{r message=FALSE}
#subset the data into train and test
n = nrow(train_joined)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
dataT = train_joined[trainIndex ,]
dataV = train_joined[-trainIndex ,]
```

# 7. Step 6. Build the model

### 7.1 The choice of the model

The choice of forecasting technique or model to use is a major consideration. There is a multiplicity of different methods to apply, from pure guesswork to highly mathematical statistical analysis [forecasting for sales].

The choice is always a compromise between accuracy, time-scale and cost. To make sure that the best use is made of the available information, the assumptions used in formulating the forecast must be clearly understood by both the forecaster and the decision-maker who uses the forecast [Forecasting for sales].

In this particualr case we will have use a multiple regression approach for the following reasons:

- Multiregression is more understandable for non-specialist with a knowledge of forecasting. Multi regression is a techique that is simpler, easier to understand and also more accessible to people with no background in maths and forecasting.

- Rather than use a complex relationship between the forecast variable and an independent variable (which has also to be guessed), it is simpler, and quite probably more accurate, merely to guess the forecast. It will usually be cheaper and quicker as well.

Multiple regression analysis is a multivariate statistical technique for examining the linear correlations between two or more independent variables and a single dependent variable.

### 7.2 Build the initial model

```{r}
# Fit the model (1)
fit <- lm(Weekly_Sales ~., data= dataT)
summary(fit) #R2 = 96.7%
```

The item that is to be forecast is known as the
dependent variable, while the data used to produce the forecast must
consist of independent variables. Independent

### 7.3 Fit the model (or choose the right variables)

Check for multi-collinearity with Variance Inflation Factor

Correlated: none VIF=1, moderately 1<VIF<5, ** highly 5<VIF<10, ...
```{r}
# Variance Inflation Factor
vif(fit)
```

```{r}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ old_sales + `Size (sq ft)`, data= dataT)
summary(fit) #R2 = 96.7%
```

```{r}
# Variance Inflation Factor
vif(fit)
```
VIF, F-ratio and p-values say it is good, so no need to do anything else.

Monoscedasticity (even distribution of residuals)

p-value of coefficients and R2?F statistic of the model

### 7.4 The final model

```{r}
# Refit the model (2) - drop IsHoliday, Temperature, Fuel Price, CPI and Unemployment
fit <- lm(Weekly_Sales ~ old_sales + `Size (sq ft)`, data= dataT)
summary(fit) #R2 = 96.7%
```

# 8. Step 7. Evaluation of the model

```{r}
#     Find all predicted values for both a training set and a validation set
dataT$Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(dataT, select=c(old_sales, `Size (sq ft)`)))
dataV$Pred.Weekly_Sales <- predict(fit, 
    newdata = subset(dataV, select=c(old_sales, `Size (sq ft)`)))
```

```{r}
# The theoretical model performance is defined here as R-Squared
summary(fit)
```

```{r}

# Check how good is the model on the training set - correlation^2, RME and MAE
train.corr <- round(cor(dataT$Pred.Weekly_Sales, dataT$Weekly_Sales), 2)
train.RMSE <- round(sqrt(mean((dataT$Pred.Weekly_Sales -  dataT$Weekly_Sales)^2)))
train.MAE <- round(mean(abs(dataT$Pred.Weekly_Sales - dataT$Weekly_Sales)))
c(train.corr^2, train.RMSE, train.MAE)
```

```{r}
# Check how good is the model on the validation set - correlation^2, RME and MAE
valid.corr <- round(cor(dataV$Pred.Weekly_Sales, dataV$Weekly_Sales), 2)
valid.RMSE <- round(sqrt(mean((dataV$Pred.Weekly_Sales - dataV$Weekly_Sales)^2)))

valid.MAE <- round(mean(abs(dataV$Pred.Weekly_Sales - dataV$Weekly_Sales)))
c(valid.corr^2, valid.RMSE, valid.MAE)
```

# 9. Conclusions (Interpret results)

<summary of all the steps followed>
This report builds a model for the prediction os sales. 

<conclusive points>
The first and more conclusive argument is that: 
- The best predictor of sales is the sales from the prior year.
- The second better was the size of the store.

Line up important weeks - for example predict thanksgiving by thanksgiving regardless of which week of the year it is.

<Las statement>
