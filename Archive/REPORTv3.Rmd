---
title: "Understanding Data and its Environment: Report assessment"
author: "Eugeni Vidal"
date: March 15, 2018
output:
  pdf_document: default
  word_document: default
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
  slidy_presentation:
bibliography: references.bib
---

# 1. Introduction

<Aim>
This report aims to describe, pre-process and analyse a set of data based on historical sales data collected from a nationwide retailer in the U.S, so as to lead to the development of accurate predictive models. 

<Method>
The methodology is divided into eight different steps, from describing the data to building and assessing the models developed. Each step is taken in order, although the whole process has to be understood as a set of nested loops rather than a straight line (see figure 1).

![**Figure 1. Methodology diagram**](images/Methodology diagram.png)

The whole process has been done using the software R and it is reproducible based on code available at https://github.com/eugenividal/Understanding-data-report.

# 2. Step 1: data description

```{r, include=FALSE, cache=FALSE}
library(tidyverse)
# load data
source("code/load-data.R")
```

We have been provided with 4 data sets (stores, features, train, and test). All of them with the same format: comma-separated values (csv). 

<Describe the type of each variable (categorical, numeric, etc)?>

**stores.csv (45 obs. of 3 variables)**

- Store: the anonymised store number <numeric> 
- Type: tore type, A: supercentre, B: superstore, C: supermarket <character>
- Size: store size (in square feet) <numeric>

**features.csv (8,190 obs. of 12 variables)**

- Store: the anonymised store number <numeric>
- Date:	the week with the dated Friday <character>
- Temperature: average temperature in the region <numeric>
- Fuel_Price: cost of fuel in the region <numeric>
- Promotions:	anonymised data related to promotions, mainly price reductions that the retailer is running <numeric>
- CPI: the consumer price index <numeric>
- Unemployment: the unemployment rate <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**train.csv (421,570 obs. of 5 variables)**

- Store: the anonymised store number <numeric>
- Department: the anonymised department number <numeric>
- Date: the week with the dated Friday <character>
- Weekly_Sales: sales for the given department in the given store <numeric>
- IsHoliday: whether the week is a special holiday week <boolean>

**test.csv (115,069 obs. of 5 variables)**

The validation dataset has the same fields as the train.csv, except we need to predict the weekly sales for each triplet of store, department, and date from 02/11/2012 to 26/07/2013.

<br>

After loading the data into the R environment, variables of each dataset have been explored through counts, summary statistics, crosstabulation and visualisations.

The potential inconsistences or problems identified are the following:

1. In the stores dataset, some stores might be wrongly classified. Two stores under 50,000 sq ft are coded as type B and other two as type A, when presumably small size stores (<50000 sq ft) should be type C. In addition, some stores are exactly the same size: three are 39690 sq ft, three 39910, and two 203819.


```{r, include=FALSE, cache=FALSE}
colnames(stores)[3] <- "Size"
```

```{r}
# crosstab Size and Type store data set
crosstab_Size_Type <- table(stores$Size, stores$Type)
# print first 10 rows of mydata
head(crosstab_Size_Type, n=10)
```

2. In the features dataset, 24040 values are missing, almost 50% in the promotion variables and 7% in the `CPI`, `Unemployment` and `IsHoliday`ones. To know how many NA we have and in which variables we use the code below:

```{r}
# Count number of TRUEs
sum(is.na(features))
# Find missing values
summary(features)
```

3. Some of the values in the promotions variables are negative: 4 in `Promotion1`, 25 in `Promotion2`, 13 in `Promotion3`, and 2 in `Promotion5`. Also within the `Weekly Sales` (1286 of 421571 (0.3%)) variable in the train dataset.

4. The  class of the `Date` variable within the train, features and test datasets is character instead of date. This doesn't allow to sort the data properly. Also in terms of class, it would be useful to have the variable IsHoliday in numeric type (1 or 0) instead of boolean (true or false). 

5. Finally, the dates are all given in an English date format, despite being from an American company. It is important to ensure that variables are formatted in the same way in in order to give consistency. 

This exercise of data exploration is fundamental for the usefulness of later analysis (Berry, 2004, p.65). All these problems could potentially have an impact on prediction if we do not deal with them in an appropriate way (see section 4).

# 3. Step 2: data integration

The next step is to join the stores, train and features datasets in a single one. The test dataset will be left aside to gauge the effectiveness of the models once build.

To performance the join, we use the `dplyr` package and the type of join will be left. We chose a left join because we want to end up having the same number of rows as the train dataset.  

First, we link the stores dataset with the train one, using the common attribute `Store`; then the resultant dataset with the features dataset, using the common attributes `Store`, `Date` and `IsHoliday`. See the code below.

```{r}
library(dplyr)
# join store-level data onto training dataset (so we know size and type)
train_joined = left_join(train, y= stores)
# Join train_joined onto features dataset (so we know the rest of variables)
train_joined = left_join(train_joined, y= features)
```

The result is a data frame that contains 421,570 observations of 17 variables. Notice that 1,755 observations don't match between train and features. This is because we did a left join and the features datasets collect data until 2013-07-26 while the train dataset only until 2013-07-26. This way, we leave behind 1,755 observations and the 585 NA's previously detected in `CPI`, `Unemployment` and `IsHoliday`.

# 4. Step 3: fixing the problems with the data

In this section, we will fix inconsistencies or potential problems detected in the section data description.

<What is or isnâ€™t a problem varies with the data mining technique used (Berry, 2004, p.72).> 

### Inconsistent Data Encoding

We will assume that the type of store is based on size and consequently, will recode store As and Bs under 50000 sq ft as C type. 

```{r}
# recode stores type based on size
train_joined$Type[(train_joined$Size<50000)] <- "C"
train_joined$Type[(train_joined$Size>=50000) & (train_joined$Size<150000)] <- "B"
train_joined$Type[(train_joined$Size>=150000)] <- "A"
```

However, we understand reasonable that certain stores have exactly the same size. The company could have built them from scratch, taking others as a model. 

### Missing values

Given that promotion variables have almost 50% of their values missing (N.A), we will not consider and remove them from the training   dataset<(reference needed)>. 

```{r}
# remove promotion columns
train_joined$Promotion1 <- NULL
train_joined$Promotion2 <- NULL
train_joined$Promotion3 <- NULL
train_joined$Promotion4 <- NULL
train_joined$Promotion5 <- NULL
```

### Negative values

The negative values in `Weekly Sales` could be attributed to periods in which stores spent more than they earned planning future sales. <(reference needed)>. 

### Impractical class type

We will change the class type of `IsHoliday` variable into numeric using the general function `as.numeric()`.

```{r}
# convert IsHoliday to numeric
train_joined$IsHoliday <- as.numeric(train_joined$IsHoliday)
```

To convert the date information into date we will use the package `lubridate` and the function `mdy ()`.

```{r message=FALSE}
library(lubridate)
# convert date info in format 'dmy'
train_joined$Date <- dmy(train_joined$Date)
```

### unconsistent formats

Finally, to give consistency to the dataset we will convert the date from English format (dmy) to American(mdy).

# 5. Step 4: data transformation

Once the inconsistencies or potential problems are fixed, we will make some transformations in order to prepare the dataset for building the predictive model.

### Creating variables

First, we include a new column with Week Number of the year column. 

```{r}
# week number of the year
train_joined$WeekNum <- as.numeric(format(train_joined$Date+3,"%U"))
```

Second, we create a unique identifier column by concatenating store number and department type.

```{r}
# create a unic identifier
train_joined$Id <- as.character(paste(train_joined$Store, train_joined$Dept, sep = "-"))
```

Could be also intereting to include other colums such as Weekely sales of previous week or previous year.

<Create a variable for instance of sales/square meters?>

### Data reduction

Some variables such as `Type` and `Size` give very similar information (pressumably, size is based on type), although `Size` is much more precise and numerial. We will consider in removing the `Type` variable.

At this point, using the `head()` function we can have a glimpse of how the traning dataset looks like.
```{r, include=FALSE, cache=FALSE}
# move location of columns
train_joined <- train_joined[c(13,2,1,6,7,3,12,4,5,8,9,10,11)]
```

```{r}
# print first 6 rows of mydata
head(train_joined)
```
 
# 6. Step 5: data partition

In this section, the model set is divided into two parts. The first one, the training set (train_joined.train), will be used to build the initial models; and the second one, the validation set (train_joined.validation), to adjust them. The test set that we put aside before, will be used to gauge the effectiveness of the models when applied to unseen data. 

<why 80%? (reference needed)>

For the partition, we will use the `caret` package.

```{r message=FALSE}
library(caret)
# subset the data into train and test
n = nrow(train_joined)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
train_joined.train = train_joined[trainIndex ,]
train_joined.validation = train_joined[-trainIndex ,]
```

We can check that the data has been subsetted with the following command:

```{r}
ls()
```

# 7. Step 6: identify the key factors affecting sales

Up to this point, we wonder which variables are most important in predicting the outcome?

If predictor variable were uncorrelated this would be a simple task. You would rank-over the predictor variable by their correlation with the response variable. However, in most cases, the predictors are correlated with each other, and this complicates the task.

### Linear regression

```{r, echo=FALSE}
fit1<- lm(Weekly_Sales ~ Size, data = train_joined.train)
summary(fit1)
library(ggplot2)
ggplot(train_joined.train, aes(x=Size, y=Weekly_Sales)) + geom_point(aes(color=Type)) + geom_smooth(method='lm') + labs(title = "weekly sales vs Size", x = "Size", y = "Sales", color = "Type of store")
```

```{r, echo=FALSE}
fit2 <- lm(Weekly_Sales ~ Temperature, data = train_joined.train)
summary(fit2)
ggplot(train_joined.train, aes(x=Temperature, y=Weekly_Sales)) + geom_point(aes(color=Type)) + geom_smooth(method='lm') + labs(title = "weekly sales vs Temperature", x = "Temperature", y = "Sales", color = "Type of store")
```

```{r, echo=FALSE}
fit3 <- lm(Weekly_Sales ~ Fuel_Price, data = train_joined.train)
summary(fit3)
ggplot(train_joined.train, aes(x=Fuel_Price, y=Weekly_Sales)) + geom_point(aes(color=Type)) + geom_smooth(method='lm') + labs(title = "weekly sales vs Fuel Price", x = "Fuel Price", y = "Sales", color = "Type of store")
```

```{r, echo=FALSE}
fit4 <- lm(Weekly_Sales ~ CPI, data = train_joined.train)
summary(fit4)
ggplot(train_joined.train, aes(x=Fuel_Price, y=Weekly_Sales)) + geom_point(aes(color=Type)) + geom_smooth(method='lm') + labs(title = "weekly sales vs CPI", x = "CPI", y = "Sales", color = "Type of store")
```

```{r, echo=FALSE}
fit5 <- lm(Weekly_Sales ~ Unemployment, data = train_joined.train)
summary(fit5)
ggplot(train_joined.train, aes(x=Unemployment, y=Weekly_Sales))+ geom_point(aes(color=Type)) + geom_smooth(method='lm') + labs(title = "weekly sales vs Unemployment", x = "Unemployment", y = "Sales", color = "Type of store")
```

In the previous graphs, we can see that there is no clear correlation between the dependent variable `Weekly Sales` and the rest of quantitative predictors. 

However what is clear from this analysis is that Type A stores have more sales than any other type.

### Computing the correlation matrix

```{r}
numericdataset <- train_joined.train[,-c(1,2,3,4,6,7,9 ),drop=FALSE]
corr <- round(cor(numericdataset), 1)
head(corr[,])
```

### Correlation matrix visualization

For the visalization we will use the package `ggcorrplot`

```{r}
library(ggcorrplot)
# Drawing a heatmap
# Visualize the correlation matrix
# --------------------------------
# method = "square" (default)
ggcorrplot(corr)
```

Negative correlations are shown in blue and the positive ones in red.

### Multiple regression

```{r}
fit <- lm(Weekly_Sales ~ Size + Temperature + Fuel_Price + CPI + Unemployment,
            data= train_joined.train)
summary(fit)
```

# 8. Step 7: build the models

After identified a number of input variables for predicting the output, different predictive models, such as regression models, NN, can be easily implemented through using data mining software. Consider in using SAS to builkd the model.


# 9. Step 8: assess the models

In this section, we determine whether or not the models are working.
<we need to do the join in the test dataset>

# 10. Conclusions

Among others, we should, in the conclusions, report the limitations of the data or what future studies of the same topic might need to look for.


# References